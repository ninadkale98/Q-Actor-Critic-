{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda  in use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13476/4139666265.py:24: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f1077509dd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\" \n",
    "else:\n",
    "  device = \"cpu\"\n",
    "print(device, \" in use\")\n",
    "\n",
    "# Utility librarues\n",
    "import base64, io\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "obs_space = 4\n",
    "action_space = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs_space, 128)\n",
    "        self.hidden = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, action_space) # action probabilities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.hidden(x))\n",
    "        action_prob = F.softmax(self.out(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs_space + action_space, 128)\n",
    "        self.hidden = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1) # action probabilities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.hidden(x))\n",
    "        Q_val = F.relu(self.out(x))\n",
    "        return Q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_action(obs, Actor_model):\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    probs = Actor_model(obs)\n",
    "    dist = Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    return action.item(), log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_Qsa(obs, action, Qsa_model):\n",
    "    action_arr = np.eye(action_space)[action].reshape(-1)\n",
    "    inp = np.concatenate((obs, action_arr))\n",
    "    inp = np.reshape(inp, -1)\n",
    "    inp = torch.from_numpy(inp).float()\n",
    "    Qval = Qsa_model(inp)\n",
    "    return Qval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actor_model = Actor()\n",
    "Qsa_model = Q_Critic()\n",
    "Actor_lr = 0.001\n",
    "Actor_optim = optim.Adam(Actor_model.parameters(), lr = Actor_lr)\n",
    "Qsa_lr = 0.001\n",
    "Qsa_optim = optim.Adam(Qsa_model.parameters(), lr = Qsa_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_itr = 1000\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m Qsa_optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m Critic_loss_T \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(Critic_loss)\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> 30\u001b[0m Critic_loss_T\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     31\u001b[0m Qsa_optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "## Reset\n",
    "obs, info = env.reset()\n",
    "terminated, truncated = False, False\n",
    "\n",
    "# for one episode\n",
    "Actor_loss = []\n",
    "Critic_loss = []\n",
    "Qsa_prev = 0\n",
    "for i in range(max_itr):\n",
    "    action, log_prob = give_action(obs, Actor_model)\n",
    "    Qsa = give_Qsa(obs, action, Qsa_model)\n",
    "    Actor_loss.append( - log_prob*Qsa )\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    delt = reward + gamma*Qsa - Qsa_prev\n",
    "    Critic_loss.append( delt*Qsa )\n",
    "    Qsa_prev = Qsa\n",
    "\n",
    "    if truncated or terminated:\n",
    "        break\n",
    "\n",
    "#Backpropagation\n",
    "\n",
    "Actor_optim.zero_grad()\n",
    "Actor_loss_T = torch.stack(Actor_loss).sum()\n",
    "Actor_loss_T.backward()\n",
    "Actor_optim.step()\n",
    "\n",
    "Qsa_optim.zero_grad()\n",
    "Critic_loss_T = torch.stack(Critic_loss).sum()\n",
    "Critic_loss_T.backward()\n",
    "Qsa_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f10763d1e50>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
